{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Through Email - Part 1 Read, cache, analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using jupyter/all-spark-notebook Docker image, this assumes the following startup. See [Connecting to A Spark Cluster In Standalone Mode](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/specifics.html#apache-spark) for details.\n",
    "\n",
    "```bash\n",
    "docker run -p 8888:8888 -v /datasets/enron:/datasets/enron \\\n",
    "   -v /home/jovyan/work:/home/jovyan/work \\\n",
    "   --net=host --pid=host -e TINI_SUBREAPER=true \\\n",
    "   -e SPARK_OPTS='--master=spark://{spark-master-routable-ip}:{port} --executor-memory=8g' \\\n",
    "   jupyter/all-spark-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read email from file in Apache Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "records = [uuid: string, from: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[uuid: string, from: string ... 8 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val records = spark.read.parquet(\"/datasets/enron/enron-small.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache dataset for repeated exploration\n",
    "\n",
    "Lazy evaluation of transformations vs. action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[uuid: string, from: string ... 8 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read was a transformation - nothing executed yet\n",
    "records.cache //a transformation - nothing gets executed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191926"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.count //an action - actually read data and perform \"cache\" transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records now cached in memory on Spark standalone executor(s) - see http://localhost:8080/ - Running Application - Application Detail UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cc: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- bcc: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- dateUtcEpoch: long (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- mailFields: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- attachments: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- fileName: string (nullable = true)\n",
      " |    |    |-- size: integer (nullable = true)\n",
      " |    |    |-- mimeType: string (nullable = true)\n",
      " |    |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who are the Top 10 email message senders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----+\n",
      "|from                         |count|\n",
      "+-----------------------------+-----+\n",
      "|vince.kaminski@enron.com     |14340|\n",
      "|jeff.dasovich@enron.com      |10888|\n",
      "|chris.germany@enron.com      |8688 |\n",
      "|steven.kean@enron.com        |6722 |\n",
      "|sally.beck@enron.com         |4253 |\n",
      "|john.arnold@enron.com        |3505 |\n",
      "|david.delainey@enron.com     |2991 |\n",
      "|enron.announcements@enron.com|2803 |\n",
      "|pete.davis@enron.com         |2753 |\n",
      "|phillip.allen@enron.com      |2145 |\n",
      "+-----------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fromsWithCountsDesc = [from: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[from: string, count: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\n",
    "import org.apache.spark.sql.functions.desc\n",
    "\n",
    "//lazy transformations\n",
    "val fromsWithCountsDesc = records.select(\"from\").\n",
    "   groupBy(\"from\").\n",
    "   count.\n",
    "   orderBy(desc(\"count\")).\n",
    "   limit(10)\n",
    "\n",
    "//action\n",
    "fromsWithCountsDesc.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[uuid: string, from: string ... 8 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kill the Spark application that was created for running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
