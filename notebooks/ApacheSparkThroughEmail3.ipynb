{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Through Email 3 - Explode, Shuffle Partitions, UDF, Parquet partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "records = [uuid: string, from: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "191926"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val homeDir = sys.props(\"user.home\")\n",
    "val records = spark.read.parquet(\"/datasets/enron/enron-small.parquet\")\n",
    "records.cache\n",
    "\n",
    "//8 cached partitions\n",
    "records.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What user has the most emails in his/her mailbox?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// |-- mailFields: map (nullable = true)\n",
    "//|    |-- key: string\n",
    "//|    |-- value: string (valueContainsNull = true)\n",
    "records.printSchema\n",
    "\n",
    "/*\n",
    "root\n",
    " |-- key: string (nullable = false)\n",
    " |-- value: string (nullable = true)\n",
    " */\n",
    "records.select(explode(records(\"mailFields\"))).printSchema\n",
    "\n",
    "records.select(explode(records(\"mailFields\"))).show(10)\n",
    "\n",
    "//explode of map creates columns \"key\" and \"value\"\n",
    "val userMailFields = records.select(explode($\"mailFields\")).\n",
    "    where($\"key\" === \"UserName\").withColumnRenamed(\"value\",\"userName\").select(\"userName\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191926"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//sanity check\n",
    "userMailFields.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultShufflePartitions = 200\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val defaultShufflePartitions = spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions before shuffle: 8\n",
      "Number of partitions after shuffle: 200\n",
      "Number of partitions after limit: 1\n",
      "+----------+-----+\n",
      "|  userName|count|\n",
      "+----------+-----+\n",
      "|kaminski-v|28465|\n",
      "|dasovich-j|28234|\n",
      "|    kean-s|25351|\n",
      "| germany-c|12436|\n",
      "|    beck-s|11830|\n",
      "|campbell-l| 6490|\n",
      "|  guzman-m| 6054|\n",
      "|     lay-k| 5937|\n",
      "|haedicke-m| 5246|\n",
      "|  arnold-j| 4898|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "allUserCounts = [userName: string, count: bigint]\n",
       "top10UserCounts = [userName: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userName: string, count: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// disable adaptive query engine (AQE since 3.0.0)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")  // default true\n",
    "\n",
    "//groupBy causes shuffle - default 200 partitions - ~ 2 seconds to execute\n",
    "println(s\"Number of partitions before shuffle: ${userMailFields.rdd.getNumPartitions}\")\n",
    "\n",
    "val allUserCounts = userMailFields.groupBy(\"userName\").count\n",
    "\n",
    "println(s\"Number of partitions after shuffle: ${allUserCounts.rdd.getNumPartitions}\")\n",
    "\n",
    "val top10UserCounts = allUserCounts.orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "println(s\"Number of partitions after limit: ${top10UserCounts.rdd.getNumPartitions}\")\n",
    "\n",
    "top10UserCounts.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting default partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions after shuffle: 8\n",
      "+----------+-----+\n",
      "|  userName|count|\n",
      "+----------+-----+\n",
      "|kaminski-v|28465|\n",
      "|dasovich-j|28234|\n",
      "|    kean-s|25351|\n",
      "| germany-c|12436|\n",
      "|    beck-s|11830|\n",
      "|campbell-l| 6490|\n",
      "|  guzman-m| 6054|\n",
      "|     lay-k| 5937|\n",
      "|haedicke-m| 5246|\n",
      "|  arnold-j| 4898|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "allUserCountsSmall = [userName: string, count: bigint]\n",
       "top10UserCountsSmall = [userName: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userName: string, count: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "\n",
    "val allUserCountsSmall = userMailFields.groupBy(\"userName\").count\n",
    "\n",
    "println(s\"Number of partitions after shuffle: ${allUserCountsSmall.rdd.getNumPartitions}\")\n",
    "\n",
    "val top10UserCountsSmall = allUserCountsSmall.orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "//executes in about 0.3s\n",
    "top10UserCountsSmall.show\n",
    "\n",
    "// now - leave default AQE enabled\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")  // default true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing partitioned data with UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data access patterns: By using directory structures for our typical queries we can \"index\" data for faster access.\n",
    "Let's say that for our analysis we only want \"enron.com\" senders and typically access the data by \"from\" field (and we have a *lot* of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191926\n",
      "154101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isEnronEmailUdf = UserDefinedFunction(<function1>,BooleanType,Some(List(StringType)))\n",
       "enrons = [uuid: string, from: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "isEnronEmail: (str: String)Boolean\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[uuid: string, from: string ... 8 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def isEnronEmail(str: String): Boolean = str.endsWith(\"@enron.com\")\n",
    "\n",
    "val isEnronEmailUdf = udf(isEnronEmail(_:String):Boolean)\n",
    "\n",
    "println(records.count)\n",
    "\n",
    "val enrons = records.where(isEnronEmailUdf($\"from\"))\n",
    "\n",
    "println(enrons.count)\n",
    "\n",
    "enrons.write.partitionBy(\"from\").parquet(s\"$homeDir/datasets/enron/from-part\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kaminskis = [uuid: string, to: array<string> ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14340"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kaminskis = spark.read.parquet(\"/datasets/enron/from-part\").where(\"from = 'vince.kaminski@enron.com'\")\n",
    "\n",
    "kaminskis.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
